{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd8d11d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "from gym import wrappers\n",
    "from gym.envs.registration import register\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8dd88b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_the_game():\n",
    "    \"\"\"to show how to run a env\"\"\"\n",
    "    done = False\n",
    "    env.reset()\n",
    "    display(env.render())\n",
    "    while not done:\n",
    "        random_action = env.action_space.sample()\n",
    "        \"\"\"\n",
    "        observation：进入的新状态\n",
    "        reward：采取这个行动得到的奖励\n",
    "        done：当前游戏是否结束\n",
    "        info：其他一些信息，如性能表现，延迟等等，可用于调优\n",
    "        \"\"\"\n",
    "        new_state, reward, done, info = env.step(\n",
    "            random_action\n",
    "        )\n",
    "        clear_output(wait=True)\n",
    "        display(env.render())\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48fd5df",
   "metadata": {},
   "source": [
    "> Discounted sum of rewards from time step t to horizon<br>\n",
    "    $G_t = R_{t+1} + γR_{t+2} + γ^2R_{t+3} + γ^3R_{t+4} + ... + γ^{T−t−1}R_T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6739f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy, gamma=1.0, render=False):\n",
    "    \"\"\"\n",
    "    run this game\n",
    "    0: left\n",
    "    1: down\n",
    "    2: right\n",
    "    3: up\n",
    "    \"\"\"\n",
    "    obs = env.reset() # state initialization\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    \n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        \n",
    "        obs, reward, done, _ = env.step(int( policy[obs] ))\n",
    "        \n",
    "        # MDP algorithms\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73554025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, gamma = 1.0, n=100):\n",
    "    \"\"\"take the average of 100 samples\"\"\"\n",
    "    scores =[ run_episode(env, policy, gamma, False) for _ in range(n)]\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c89ae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policy(v, gamma=1.0):\n",
    "    \"\"\"Extract the policy given a value-function\"\"\"\n",
    "    policy = np.zeros(env.env.nS) # 16\n",
    "    \n",
    "    for s in range(env.env.nS):\n",
    "        q_sa = np.zeros(env.env.nA)\n",
    "        \n",
    "        for a in range(env.env.nA):\n",
    "            q_sa[a] = sum( [p*(r + gamma*v[s_]) for p, s_, r, _ in env.env.P[s][a]] )\n",
    "        \n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "289f2664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_policy_v(env, policy, gamma=1.0):\n",
    "    v = np.zeros(env.env.nS)\n",
    "    eps = 1e-10 # greedy parameters\n",
    "    while True:\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.env.nS):\n",
    "            policy_a = policy[s]\n",
    "            v[s] = sum( [p*(r+gamma*prev_v[s_]) for p, s_, r, _ in env.env.P[s][policy_a]] )\n",
    "        if (np.sum(np.fabs(prev_v - v)) <= eps):\n",
    "            break\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75191df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma=1.0):\n",
    "    \"\"\"policy-Iteration algorithm\"\"\"\n",
    "    policy = np.random.choice(env.env.nA, size=(env.env.nS)) # initialize a random policy 随机生成策略作为初始值\n",
    "    max_iteration = 2*10^5\n",
    "    \n",
    "    for i in range(max_iteration):\n",
    "        old_policy_v = compute_policy_v(env, policy, gamma)\n",
    "        new_policy = extract_policy(old_policy_v, gamma)\n",
    "        if (np.all(policy == new_policy)):\n",
    "            print('Policy-Iteration converged at step %d.' % (i+1))\n",
    "            break\n",
    "        policy = new_policy\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73909fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy-Iteration converged at step 6.\n",
      "Average scores =  0.74\n"
     ]
    }
   ],
   "source": [
    "if __name__ =='__main__':\n",
    "    env = gym.make('FrozenLake-v0')\n",
    "    optimal_policy = policy_iteration(env, gamma=1.0)\n",
    "    scores = evaluate_policy(env, optimal_policy, gamma=1.0)\n",
    "    print('Average scores = ', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c2b3ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 3., 3., 3., 0., 0., 0., 0., 3., 1., 0., 0., 0., 2., 1., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "928514be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_episode(env, policy=optimal_policy, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce91828c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
